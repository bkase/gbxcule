# Cross-cutting engineering rules for Epics 6–9

These rules apply to _all_ work in 6–9, and are the main “correctness by construction” levers.

## A. “Functional core, imperative shell” boundaries

**Pure (core) code** should be:

- config parsing / normalization
- run-id & schema generation
- action generation / replay
- diff computation (ref vs dut)
- JSON serialization/canonicalization

**Imperative shell** should be:

- backend implementations (`pyboy_*`)
- process management (MP workers)
- IO to `bench/runs/*`
- PyBoy stepping
- CLI plumbing

Where possible: structure the harness as:

`CLI args -> RunConfig (frozen dataclass) -> run_benchmark/run_verify (pure-ish orchestrator) -> backend.step (impure) -> RunResult -> write_artifacts (impure)`

## B. Stable schemas (types + versioning)

Define:

- `RESULT_SCHEMA_VERSION = 1` for benchmark outputs
- `MISMATCH_SCHEMA_VERSION = 1` for mismatch metadata bundle

Treat schema bumps as **breaking changes** with tests.

## C. Reproducibility invariants (MUST)

A verification run’s mismatch must be reproducible from:

- ROM SHA256
- backend names + versions (commit SHA)
- seeds
- `frames_per_step`, `release_after_frames`
- `compare_every`, `verify_steps`
- action generator name/version OR action trace file (prefer both)

This implies: **always record actions** (even if noop) into a trace file.

## D. Gate latency budget (≤ 2 minutes CPU-only)

To protect the 2-minute gate:

- Precommit verify uses **1 ROM** and **small steps** (e.g., 256–1024)
- MP backend tests must avoid long spins and must enforce timeouts via joins/guards
- Scaling runs should not run in `make check` (only in `make bench`)

## E. Structured outputs only (logs + artifacts)

- On success: minimal stdout
- On failure: short structured-ish summary with a path + trace/run id
- All durable detail goes to JSON artifacts

---

# Dependency graph (so you can parallelize)

**Epic 6 (pyboy_vec_mp)** depends on:

- Epic 3: `VecBackend` protocol + shared dataclasses
- Epic 4: ROM suite generation (for smoke tests)
- Epic 5: `pyboy_single` (not strictly required, but used for parity & comparison)

**Epic 7 (Harness CLI)** depends on:

- Epic 3: types + backend registry contract
- Epic 4: suite.yaml + ROM output paths
- Epic 5 + Epic 6: backends callable by name

**Epic 8 (Verify + mismatch bundles)** depends on:

- Epic 7: CLI orchestration + run artifact writing conventions
- Epic 3: canonical state normalization helpers (or you add them now)

**Epic 9 (Docs + devlog)** depends on:

- Epics 6–8 producing real commands + artifact paths

---

# Epic 6 — `pyboy_vec_mp` CPU multiprocessing baseline backend

## Epic intent

Ship an _honest_ CPU baseline to test the core hypothesis later. It must:

- scale to N envs across `num_workers`
- be deterministic enough for benchmarking
- implement the backend contract fully (even if verification won’t use MP as oracle)
- be robust (no zombie workers, no hangs)

## Deliverables

### Code

- `src/gbxcule/backends/pyboy_vec_mp.py`
  - Implements `VecBackend` protocol
  - Worker process logic (in-module, top-level functions for spawn pickling)
  - Deterministic env seeding policy
  - Optional `get_cpu_state(env_idx)` RPC to fetch register snapshot

### Tests / smoke

- A reliability test that:
  - starts MP backend
  - steps a small number of steps
  - closes cleanly
  - fails fast if a worker dies or hangs

## Spec-level decisions (locked by architecture doc)

### Step semantics (MUST)

- `frames_per_step` default 24
- Action semantics: press at start, release after `release_after_frames` (default 8), then tick remaining frames

### Deterministic seeding (MUST)

Define (and record) seed derivation rule:

- `base_seed` is provided at `reset(seed=...)`
- per-env seed = `hash64(base_seed, env_idx, rom_sha)` (pure function, stable)

Even if PyBoy doesn’t expose “seed” universally, we still:

- deterministically initialize our action generator
- record all actions
- stabilize run configs

## Engineering plan by story

### Story 6.1 — Implement `pyboy_vec_mp` backend

#### Interface contract checklist (make invalid states unrepresentable)

Create a frozen config dataclass used by the backend, e.g.:

- `PyBoyMpConfig(num_envs: int, num_workers: int, frames_per_step: int, release_after_frames: int, rom_path: Path, base_seed: int | None, headless: bool = True, ...)`

Validation in `__post_init__` / constructor:

- `num_envs >= 1`
- `1 <= num_workers <= num_envs`
- `0 <= release_after_frames <= frames_per_step`

Fail early with clear errors.

#### Process model (minimal, robust)

Implement a **master + workers** model:

- Master partitions env indices across workers (static partition).
- Each worker owns `k` PyBoy instances (k = envs assigned).
- Master sends “commands” to workers:
  - `RESET(base_seed)`
  - `STEP(actions_chunk)` where actions_chunk is actions for worker’s env slice
  - `GET_CPU_STATE(local_env_idx)`
  - `CLOSE`

Worker responses:

- `OK(step_count, maybe_error=None)`
- `STATE(cpu_state_dict)`
- `ERR(error_type, message, traceback?)`

Design goals:

- **No per-step large transfers** (obs can be synthesized in master as zeros for M0)
- All exceptions inside worker are marshaled back and cause master to fail fast.

#### Action handling

Even if micro-ROMs ignore input, implement the action pipeline end-to-end:

- Define canonical action dtype: `np.int32[num_envs]`
- Action mapping table kept in one place (small enum-like mapping)
- Worker applies press/release around stepping frames

#### `get_cpu_state(env_idx)`

Implement as:

- Master routes request to owning worker
- Worker returns normalized dict with stable keys:
  - `pc, sp, a,f,b,c,d,e,h,l` (ints)
  - `flags` derived from `f`
  - counters if available else `null`

If PyBoy cannot provide a field on a platform/version:

- raise a clear exception (don’t silently omit unless you’ve standardized “null” and test it)

#### Failure modes to handle explicitly

- worker dies => master detects broken pipe and raises
- worker returns ERR => master raises and closes remaining workers
- close must always join processes (bounded wait + terminate fallback)

#### Performance posture (M0)

This is a _baseline_; do not over-optimize yet. But do avoid self-inflicted slowdown:

- Avoid returning obs every step (return zeros)
- Avoid returning info blobs per step (keep minimal)
- Keep IPC payload small (actions + ack)

### Story 6.2 — Basic reliability checks for MP backend

#### Required tests

Add a test (or harness-invoked smoke) that:

- generates micro-ROMs (or assumes generated)
- starts `pyboy_vec_mp(num_envs=4, num_workers=2)`
- does `reset(seed=123)`
- runs `step()` for e.g. 8–32 steps
- calls `close()`
- asserts no hangs and no leaked processes

To make this robust without extra deps:

- implement join timeouts in backend close
- in test, enforce a wall-clock max by design (keep steps tiny)

#### Determinism sanity check (benchmark-level)

Not strict bitwise determinism required here, but check:

- two runs with same config produce artifacts with the same **recorded config** and **same action trace** (if using seeded generator)
- and similar SPS (within tolerance) is optional

---

# Epic 7 — Harness CLI for benchmark + reporting

## Epic intent

Ship a single CLI (`bench/harness.py`) that:

- runs a backend on a ROM or suite
- measures throughput correctly (warmup excluded)
- writes **structured JSON** artifacts with a stable schema
- supports scaling runs (env_count sweep)
- records action generator + seed deterministically

This is the “truth machine” for later E0–E4.

## Deliverables

### Code

- `bench/harness.py` with:
  - CLI arg parsing
  - backend selection / construction
  - benchmark runner
  - scaling runner
  - artifact writer

### Output schema

- `bench/runs/<timestamp_utc>/<run_id>.json` for single runs
- optionally `bench/runs/<timestamp_utc>/<run_id>__scaling.json` for scaling sweep

## Harness architecture (unidirectional flow)

Model the run as:

`RunConfig -> (backend init) -> warmup loop -> measure loop -> RunResult -> write artifact`

No hidden global state. Keep everything in:

- `RunConfig` (frozen)
- `RunResult` (frozen)

## Story 7.1 — Implement harness CLI skeleton

### CLI surface (M0 required)

Support:

- `--backend {pyboy_single, pyboy_vec_mp}`
- `--rom PATH` or `--suite bench/roms/suite.yaml`
- `--stage emulate_only` (accept the flag; may not change behavior in M0)
- `--steps N`, `--warmup-steps W`
- `--output-dir` (default `bench/runs/`)

Backend-specific args:

- `--num-envs`
- `--num-workers` (only for mp)

Action args:

- `--actions-seed INT`
- `--action-gen {noop, seeded_random}` (optional but useful)
- `--frames-per-step INT` default 24
- `--release-after-frames INT` default 8

### Measurement protocol (MUST)

- Warmup steps excluded from timer
- Measured steps timed with monotonic clock
- For vector backends:
  - measured “env steps” = `steps * num_envs`
  - `total_env_steps_per_sec = (steps * num_envs) / seconds`
  - `per_env_steps_per_sec = steps / seconds`

- Compute `frames_per_sec = total_env_steps_per_sec * frames_per_step`

### Artifact schema (RESULT_SCHEMA_VERSION=1)

Include at minimum:

Top-level:

- `schema_version`
- `run_id`
- `timestamp_utc`

`config`:

- backend name/device
- rom id/path/sha
- stage
- steps, warmup_steps
- num_envs, num_workers
- frames_per_step, release_after_frames
- action_generator: `{name, version, seed}` (and/or file path if replay)
- sync_every (even if unused in CPU backends; keep field for future)

`system`:

- python version
- platform info
- cpu summary (stdlib)
- git commit sha if available

`results`:

- warmup_steps
- measured_steps
- seconds
- total_env_steps_per_sec
- per_env_steps_per_sec
- frames_per_step
- frames_per_sec

### Tests to write first (shift left)

Create a fake backend in tests (in-memory) that:

- has deterministic `step()` cost and returns fixed outputs
- verify harness math:
  - SPS computations are correct
  - schema fields exist
  - JSON is valid and stable in structure

Avoid timing assertions that are flaky; test formulas, not performance.

## Story 7.2 — Scaling run support (data-only)

Add:

- `--env-counts 1,2,4,8,...`

Behavior:

- For each env_count:
  - run benchmark with identical config except env count
  - append result entry

- Emit a single JSON scaling artifact with:
  - `schema_version`
  - sweep config
  - list of per-run “results” entries (can embed the per-run artifacts or store summary rows)

Keep it deterministic in ordering and field names.

## Story 7.3 — Deterministic action generation

Implement action generation as a pure function:

`generate_actions(step_idx, num_envs, seed, gen_name, gen_version) -> np.ndarray[int32]`

M0 default can be `noop` (all zeros), but still:

- record seed + generator
- optionally allow seeded pseudo-random actions for stress

Also implement **action trace recording** (even if noop):

- in benchmark mode, action trace may be omitted to avoid overhead (optional)
- in verify mode (Epic 8), action trace is required

But you can still record actions in benchmark with minimal overhead (JSONL write per step can be heavy); for M0 I’d do:

- benchmark: record generator spec only
- verify: record full actions.jsonl

(You can keep a `--record-actions` flag for benchmark if you want.)

---

# Epic 8 — Verification scaffold + mismatch repro bundle

## Epic intent

Make correctness debugging scale from day 1:

- `--verify` runs ref vs dut (ref is `pyboy_single`)
- mismatch fails fast, emits a repro bundle with action trace, states, diffs, and a one-command repro script
- **no goldens yet**: reference computed live

Even if DUT is stubbed in M0, the system must work end-to-end.

## Deliverables

- Harness `--verify` mode
- Mismatch bundle writer (atomic, JSON-only)
- A DUT placeholder backend (recommend: implement a stub mode in `warp_vec.py` so we don’t invent new module names)

## Story 8.1 — Add `--verify` mode scaffold

### Verify loop semantics

Inputs:

- `--ref-backend pyboy_single`
- `--dut-backend warp_vec` (stub for M0) or an explicit `--dut-backend stub`
- `--verify-steps N`
- `--compare-every K` (default 1)
- `--actions-seed`, `--action-gen`
- ROM and step semantics flags

Loop:

1. Initialize both backends with the same ROM + step semantics.
2. Reset both with same seed.
3. For `i in 0..N-1`:
   - generate actions (and append to actions trace)
   - `ref.step(actions)` and `dut.step(actions)`
   - if `i % compare_every == 0`:
     - `ref_state = ref.get_cpu_state(env_idx)`
     - `dut_state = dut.get_cpu_state(env_idx)`
     - normalize + compare
     - on mismatch: emit bundle, exit non-zero

M0 scope: verify env_idx = 0 only (since pyboy_single is single-env). The code should be written to generalize later.

### Comparison logic (pure)

Implement:

- `normalize_cpu_state(dict) -> dict` with:
  - stable key ordering
  - ints as Python ints
  - `flags` derived consistently

- `diff_states(ref, dut) -> diff_json` with field-level differences

No floating point here, so strict equality is fine.

### DUT stub choice (M0 practical)

To satisfy “verify expected to fail until DUT exists” **without inventing new backend names**, implement in `src/gbxcule/backends/warp_vec.py`:

- a mode `device="cpu"` stub that:
  - returns obviously wrong register state (e.g., all zeros), or
  - increments a counter only

- still implements the backend protocol so harness works

This sets you up for M1/M2 without changing harness wiring.

## Story 8.2 — Implement mismatch repro bundle writer

### Bundle path

Write to:

`bench/runs/mismatch/<timestamp>_<rom_id>_<ref>_vs_<dut>/`

Use an atomic strategy:

- write to temp dir `.../.tmp_<uuid>`
- write all files
- rename to final dir

### Bundle contents (JSON-only + shell script)

Required files:

- `metadata.json`
- `ref_state.json`
- `dut_state.json`
- `diff.json`
- `actions.jsonl` (required by architecture doc)
- `repro.sh`

Metadata must include:

- ROM path + SHA256
- backend names + devices
- env counts, steps, warmup, stage
- seed + action generator name/version
- mismatch step index + env id
- git commit SHA (if available)
- CPU summary, GPU name if cuda (even if stubbed now, keep field)
- `frames_per_step`, `release_after_frames`, `compare_every`, `sync_every`

`repro.sh` should:

- be executable
- use uv-only invocation, e.g. `uv run python bench/harness.py ...`
- prefer replaying actions from `actions.jsonl` so reproduction survives generator changes

That implies the harness should support:

- `--actions-file path/to/actions.jsonl`

If `--actions-file` is provided:

- ignore `--actions-seed` and generator
- load actions deterministically

### Console mismatch summary

On mismatch, print:

- mismatch step
- a small set of differing fields (first N keys)
- bundle path
- trace/run id

Keep it short; the bundle is the truth.

### Tests (must-have)

A test that:

- runs `bench/harness.py --verify` using:
  - ref = pyboy_single on a micro-ROM
  - dut = warp_vec stub
  - small verify steps (e.g. 4)

- asserts:
  - process exits non-zero
  - mismatch directory created
  - required files exist
  - metadata contains required keys
  - `repro.sh` references uv-only command
  - `actions.jsonl` has expected number of lines

This is the “verifiable reward” loop for Epic 8.

---

# Epic 9 — Documentation + initial marketing devlog tweet draft

## Epic intent

Make the project runnable and legible for:

- contributors (setup + commands)
- curious developers (story + what success means)
- future you (where artifacts live, how to repro mismatches)

## Deliverables

- README updates (“why + how to run”)
- Devlog tweet draft included (README section is simplest given current required repo structure)

## Story 9.1 — README: “why + how to run”

### README sections (minimum)

1. **What this is** (plain language)
   - GPU-native many-env Game Boy stepping
   - reference oracle = PyBoy

2. **The hypothesis** (H) and what counts as success (≥1.5× emulate-only at scale; ≥1.2× with reward)
3. **Install (uv-only)**
   - `uv sync`
   - `make setup`

4. **Generate micro-ROMs**
   - `make roms`

5. **Run baselines**
   - `make bench` (should run both pyboy_single + pyboy_vec_mp at least once)
   - show where JSON artifacts go

6. **Run verify (expected to fail in M0 due to DUT stub)**
   - `make verify`
   - explain mismatch bundles and how to use `repro.sh`

7. **Artifacts**
   - `bench/runs/<timestamp>/...json`
   - `bench/runs/mismatch/.../metadata.json` etc.

8. **Minimal interpretation guide**
   - SPS vs frames/sec
   - warmup vs steady-state (even if steady-state reporting is basic in M0)

Keep it tight and operational.

### “Docs as code” (M0-lite)

You don’t need full doc-tests in M0, but do:

- ensure every command in README is real and runs
- optionally: add a tiny CI-ish test that greps README for `make` commands and verifies they exist in Makefile (cheap guard)

## Story 9.2 — Devlog tweet draft

Put the draft in README under a clearly labeled section (or a small file if you decide to add `docs/` later).

Requirements checklist:

- why (hypothesis + constraints)
- where we are today (baselines + micro-ROM tests + mismatch automation exists)
- what success looks like (steady-state scaling + correctness)
- a visual (chart or terminal snippet) — in M0, you can reference a command and say “chart coming”, but ideally include one terminal output snippet once `make bench` runs.
- a reproducible command (`make bench` or `uv run python bench/harness.py ...`)

**Important:** Since M0 may not yet have real measured numbers, the plan should include:

- run `make bench` once on a clean machine
- paste the resulting `total_env_steps_per_sec` from the JSON artifact into the tweet draft (as “first numbers”)

---

# Integration plan: how Epics 6–9 fit into `make` targets and gates

## Update Make targets (as these epics land)

### `make bench` (M0 requirement)

- runs:
  - `pyboy_single` on ALU_LOOP (small steps)
  - `pyboy_vec_mp` on ALU_LOOP (small steps, a couple env counts)

- writes artifacts under `bench/runs/`

### `make verify` (M0 requirement)

- runs verify:
  - ref = pyboy_single
  - dut = warp_vec stub
  - small steps (e.g. 16) so it fails fast

- must exit non-zero (expected in M0) **but still produce a repro bundle**
  - (If you want `make check` to pass in M0, don’t include `make verify` in `make check` yet; include a “verify scaffold smoke” that asserts bundle generation and expects failure.)

### `make check` (≤2 min CPU gate)

- ruff + pyright + pytest
- micro-ROM generator sanity
- harness bench smoke (single run)
- optional: verify scaffold smoke that asserts mismatch bundle generation

---

# Risk register + mitigations (practical)

## 1) MP backend hangs / zombie processes

Mitigation:

- time-bounded joins in `close()`
- master always terminates workers on error
- tests enforce “no hang” by design (small steps, forced shutdown)

## 2) PyBoy headless/platform quirks (SDL deps, etc.)

Mitigation:

- keep headless configuration centralized
- in tests, skip gracefully if platform lacks required system deps (but ensure CI environment you care about has them)

## 3) Determinism drift (PyBoy versions, action generator changes)

Mitigation:

- always record ROM sha + commit sha
- record action generator name/version
- store `actions.jsonl` in mismatch bundles
- support `--actions-file` replay for repro

## 4) Benchmark apples-to-oranges (warmup, sync policy)

Mitigation:

- single benchmark runner shared across backends
- schema always records warmup, measured steps, and step semantics

---

# Concrete “PR slicing” so this is shippable

If you want this to land cleanly without big-bang merges, slice into PRs that each have verifiable gates:

1. **Epic 7 skeleton first** (harness runs pyboy_single, emits schema v1 JSON)
2. **Epic 6 MP backend** (backend + reliability tests; harness can now run it)
3. **Epic 7 scaling run + action generator spec** (pure functions + tests)
4. **Epic 8 verify mode + mismatch bundles** (with warp_vec stub; bundle test)
5. **Epic 9 README + tweet draft** (validated commands)

Each PR should add/adjust tests so the “agent reward” is always immediate (`exit 0`).

---

If you want, I can turn this plan into a **ticket pack** (one ticket per story sub-task) with:

- exact file-level checklists,
- exact acceptance tests (what command must pass),
- and a “fast gate budget” for each ticket so the ≤2 minute rule stays enforced.
