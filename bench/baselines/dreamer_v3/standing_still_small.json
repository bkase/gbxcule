{
  "schema_version": 1,
  "created_at": "2026-01-31T12:46:52.204332+00:00",
  "source": "bench/runs/rl/20260131_124620__dreamer_v3__red__standing_still_smoke_small2/metrics.jsonl",
  "input_kind": "jsonl",
  "env_steps_min": 384,
  "env_steps_max": 384,
  "count": 1,
  "keys": [
    "Grads/world_model",
    "Loss/continue_loss",
    "Loss/observation_loss",
    "Loss/reward_loss",
    "Loss/state_loss",
    "Loss/world_model_loss",
    "State/kl",
    "State/post_entropy",
    "State/prior_entropy",
    "ready_steps",
    "replay_ratio",
    "replay_size",
    "sps",
    "train_sps"
  ],
  "metrics": {
    "Loss/world_model_loss": 576.5415737628937,
    "Loss/observation_loss": 572.1688158512115,
    "Loss/reward_loss": 2.8725243443623185,
    "Loss/state_loss": 1.149186215363443,
    "Loss/continue_loss": 0.35104533162666485,
    "State/kl": 1.8927926155738533,
    "State/post_entropy": 12.715578816831112,
    "State/prior_entropy": 14.868827007710934,
    "Grads/world_model": 920.6107063293457,
    "replay_ratio": 0.3333333333333333,
    "replay_size": 24.0,
    "ready_steps": 24.0,
    "sps": 13.852531290316069,
    "train_sps": 19.132555406674456
  },
  "missing_keys": [
    "Loss/policy_loss",
    "Loss/value_loss",
    "Grads/actor",
    "Grads/critic",
    "action_entropy",
    "value_mean",
    "value_std",
    "adv_mean",
    "adv_std",
    "ret_p05",
    "ret_p95",
    "ret_scale"
  ],
  "threshold_pct": 0.15,
  "abs_tol": 1e-06
}
